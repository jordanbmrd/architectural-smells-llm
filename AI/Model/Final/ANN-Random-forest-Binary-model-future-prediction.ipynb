{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration pour la prédiction future\n",
        "PREDICTION_OFFSET = 3  # Prédire X versions dans le futur (modifiable)\n",
        "print(f\"Configuration: Prédiction à {PREDICTION_OFFSET} versions dans le futur\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Dataset\n",
        "df = pd.read_csv(\"./../../Dataset/Final-dataset-binary/transformers.csv\")\n",
        "print(\"Dataset original shape:\", df.shape)\n",
        "print(\"Versions uniques:\", sorted(df['version'].unique()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Créer les données pour la prédiction future\n",
        "def create_future_prediction_dataset(df, prediction_offset):\n",
        "    \"\"\"\n",
        "    Crée un dataset où les features à la version N prédisent le label à la version N+offset\n",
        "    \"\"\"\n",
        "    # Convertir les versions en format numérique pour faciliter le tri\n",
        "    df_sorted = df.copy()\n",
        "    df_sorted['version_numeric'] = df_sorted['version'].str.extract('(\\d+\\.\\d+\\.\\d+)').iloc[:, 0]\n",
        "    df_sorted = df_sorted.sort_values(['path', 'version_numeric'])\n",
        "    \n",
        "    future_data = []\n",
        "    \n",
        "    # Grouper par fichier (path)\n",
        "    for file_path in df_sorted['path'].unique():\n",
        "        file_data = df_sorted[df_sorted['path'] == file_path].copy()\n",
        "        file_data = file_data.sort_values('version_numeric')\n",
        "        \n",
        "        versions = file_data['version_numeric'].unique()\n",
        "        \n",
        "        # Pour chaque version, essayer de trouver la version future correspondante\n",
        "        for i, current_version in enumerate(versions):\n",
        "            if i + prediction_offset < len(versions):\n",
        "                future_version = versions[i + prediction_offset]\n",
        "                \n",
        "                # Données actuelles (features)\n",
        "                current_data = file_data[file_data['version_numeric'] == current_version].iloc[0]\n",
        "                \n",
        "                # Label futur (target)\n",
        "                future_data_row = file_data[file_data['version_numeric'] == future_version].iloc[0]\n",
        "                future_label = future_data_row['has_smell']\n",
        "                \n",
        "                # Créer la nouvelle lgigne\n",
        "                new_row = {\n",
        "                    'current_version': current_data['version'],\n",
        "                    'future_version': future_data_row['version'],\n",
        "                    'path': current_data['path'],\n",
        "                    'file_type': current_data['file_type'],\n",
        "                    'line_count': current_data['line_count'],\n",
        "                    'method_count': current_data['method_count'],\n",
        "                    'coupling_score': current_data['coupling_score'],\n",
        "                    'current_has_smell': current_data['has_smell'],\n",
        "                    'future_has_smell': future_label  # Notre nouveau target\n",
        "                }\n",
        "                \n",
        "                future_data.append(new_row)\n",
        "    \n",
        "    return pd.DataFrame(future_data)\n",
        "\n",
        "# Créer le nouveau dataset\n",
        "df_future = create_future_prediction_dataset(df, PREDICTION_OFFSET)\n",
        "print(f\"Nouveau dataset shape: {df_future.shape}\")\n",
        "print(f\"Nombre de paires (version actuelle -> version +{PREDICTION_OFFSET}): {len(df_future)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vérifier la distribution des nouvelles données\n",
        "print(\"Distribution du target future_has_smell:\")\n",
        "print(df_future[\"future_has_smell\"].value_counts())\n",
        "\n",
        "df_future[\"future_has_smell\"].value_counts().plot(kind=\"bar\")\n",
        "plt.title(f\"Distribution du target future_has_smell (prédiction à +{PREDICTION_OFFSET} versions)\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encoding categorical data pour le nouveau dataset\n",
        "categorical_features = [\"current_version\", \"path\", \"file_type\"]\n",
        "numerical_features = [\"line_count\", \"method_count\", \"coupling_score\"]\n",
        "additional_features = [\"current_has_smell\"]  # Ajouter l'état actuel comme feature\n",
        "target_feature = \"future_has_smell\"\n",
        "\n",
        "# Preprocessing\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "X_cat = onehot_encoder.fit_transform(df_future[categorical_features])\n",
        "\n",
        "X_num = df_future[numerical_features].values\n",
        "X_additional = df_future[additional_features].values\n",
        "\n",
        "X_combined = np.concatenate([X_cat, X_num, X_additional], axis=1)\n",
        "y = df_future[target_feature].values\n",
        "\n",
        "print(f\"Shape des features combinées: {X_combined.shape}\")\n",
        "print(f\"Shape du target: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Splitting the dataset into the Training set and the Test set\n",
        "split_idx = int(0.8 * len(df_future))\n",
        "X_train, X_test = X_combined[:split_idx], X_combined[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANN Model pour prédiction future\n",
        "# Architecture adaptée pour la prédiction temporelle\n",
        "ann_model = tf.keras.models.Sequential()\n",
        "ann_model.add(tf.keras.layers.Dense(units=128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "ann_model.add(tf.keras.layers.Dropout(0.3))\n",
        "ann_model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "ann_model.add(tf.keras.layers.Dropout(0.3))\n",
        "ann_model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "ann_model.add(tf.keras.layers.Dropout(0.2))\n",
        "ann_model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Architecture du modèle:\")\n",
        "ann_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entraînement avec validation et early stopping\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "history = ann_model.fit(\n",
        "    X_train, y_train, \n",
        "    validation_split=0.2, \n",
        "    epochs=150, \n",
        "    batch_size=64, \n",
        "    callbacks=[callback],\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation du modèle ANN\n",
        "y_pred_ann = (ann_model.predict(X_test) > 0.5).astype(int).flatten()\n",
        "print(\"=== RÉSULTATS ANN (Prédiction Future) ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ann))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_ann))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_ann))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fonction pour faire des prédictions sur de nouvelles données\n",
        "def predict_future_smell(current_data, prediction_offset=PREDICTION_OFFSET):\n",
        "    \"\"\"\n",
        "    Prédire si un smell sera présent dans le futur pour de nouvelles données\n",
        "    \n",
        "    current_data: dict contenant les features actuelles\n",
        "    prediction_offset: nombre de versions dans le futur à prédire\n",
        "    \"\"\"\n",
        "    # Créer un DataFrame temporaire\n",
        "    temp_df = pd.DataFrame([current_data])\n",
        "    \n",
        "    # Preprocessing (même pipeline que l'entraînement)\n",
        "    X_cat_new = onehot_encoder.transform(temp_df[categorical_features])\n",
        "    X_num_new = temp_df[numerical_features].values\n",
        "    X_additional_new = temp_df[additional_features].values\n",
        "    \n",
        "    X_combined_new = np.concatenate([X_cat_new, X_num_new, X_additional_new], axis=1)\n",
        "    X_scaled_new = scaler.transform(X_combined_new)\n",
        "    \n",
        "    # Prédiction\n",
        "    prediction = ann_model.predict(X_scaled_new)[0][0]\n",
        "    \n",
        "    return {\n",
        "        'prediction_probability': float(prediction),\n",
        "        'predicted_class': int(prediction >= 0.5),\n",
        "        'confidence': abs(prediction - 0.5) * 2  # Confiance entre 0 et 1\n",
        "    }\n",
        "\n",
        "# Exemple d'utilisation\n",
        "example_data = {\n",
        "    'current_version': df_future['current_version'].iloc[0],\n",
        "    'path': df_future['path'].iloc[0],\n",
        "    'file_type': df_future['file_type'].iloc[0],\n",
        "    'line_count': df_future['line_count'].iloc[0],\n",
        "    'method_count': df_future['method_count'].iloc[0],\n",
        "    'coupling_score': df_future['coupling_score'].iloc[0],\n",
        "    'current_has_smell': df_future['current_has_smell'].iloc[0]\n",
        "}\n",
        "\n",
        "prediction_result = predict_future_smell(example_data)\n",
        "print(\"=== EXEMPLE DE PRÉDICTION ===\")\n",
        "print(\"Données d'entrée:\", example_data)\n",
        "print(\"Résultat de prédiction:\", prediction_result)\n",
        "print(f\"Le modèle prédit que ce fichier {'AURA' if prediction_result['predicted_class'] == 1 else 'N\\\\'AURA PAS'} de smell dans {PREDICTION_OFFSET} versions\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Modèle de Prédiction Future des Code Smells\n",
        "\n",
        "## Modifications principales par rapport au modèle original :\n",
        "\n",
        "1. **Configuration flexible** : Variable `PREDICTION_OFFSET` pour définir le nombre de versions dans le futur à prédire\n",
        "2. **Transformation des données** : Fonction `create_future_prediction_dataset()` qui crée des paires (données version N → label version N+X)\n",
        "3. **Nouvelles features** : Inclusion de l'état actuel du smell (`current_has_smell`) comme feature supplémentaire\n",
        "4. **Architecture adaptée** : Modèle ANN avec plus de neurones pour gérer la complexité temporelle\n",
        "\n",
        "## Utilisation :\n",
        "\n",
        "1. **Modifier le décalage** : Changez `PREDICTION_OFFSET` pour prédire à un nombre différent de versions\n",
        "2. **Entraîner le modèle** : Exécutez toutes les cellules pour entraîner le modèle\n",
        "3. **Faire des prédictions** : Utilisez la fonction `predict_future_smell()` avec de nouvelles données\n",
        "\n",
        "## Exemple de prédiction :\n",
        "```python\n",
        "nouvelle_donnee = {\n",
        "    'current_version': '4.8.0',\n",
        "    'path': 'src/main/java/MonFichier.java',\n",
        "    'file_type': 'java',\n",
        "    'line_count': 150,\n",
        "    'method_count': 12,\n",
        "    'coupling_score': 0.75,\n",
        "    'current_has_smell': 1\n",
        "}\n",
        "\n",
        "resultat = predict_future_smell(nouvelle_donnee)\n",
        "print(f\"Probabilité de smell futur: {resultat['prediction_probability']:.2f}\")\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
